
# 1. Introduction
Snake is a classic game on arcades and mobile phones, and it is also a great case of reinforcement learning due to these reasons. First, it is a game with simple action space. The limited action space makes it suitable for training the reinforcement learning. Second, it is a game with immediate-observable rewards. The reinforcement learning can learn its reward clearly and instantaneously. Accordingly, the snake is greatly suitable for research for reinforcement learning.
However, the traditional snake game that only eat raspberries and circumvent walls is too monotonic that the AI can only face same situation, so we want to train an AI that can face environment with randomness. Therefore, the number of obstacles on the map and the positions are random and changing with time.

# 2. Related Research

## 2.1 Reinforcement Learning
Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.

## 2.2 Deep Learning

## 2.3 DQN
Brought forward by DeepMind, DQN is an end-to-end learning which shows a great result on Atari games by combining Neural Network and Q-Learning. Through the rewards getting from choosing actions on different observations, neural network is able to adjust the weights on each layer, and approach the optimal Q-function.
A Snake game can be expressed as a MDP, and through Bellman Equation, we can define the expected value of reward as a function, determined by different states and policies, which is called a Q-function, and in our cases, it is also a neural network trained by playing Snake itself.

# 3. A Snake Game with DQN

## 3.1 Rule of the game Snake 
A snake is keep moving on the game board, which the player can change its direction like up, down, left, right, when the snake touch the candies on the board, it will become longer, and if it touch the blocks, the player lose the game. The goal of the player is to make the longest snake before he/she lose the game.

In our special rule, the blocks will be generate randomly, where they will change the location every five runs.

A 8 $\times$ 8 board will be set as the game board, and there will be blank, snake body, raspberries, and blocks on it. 
## 3.2 Define the MDP of a Snake Game

### 3.2.1 State Set
Given 8 $\times$ 8 matrix $M_{state}$, $\forall m_{ij} \in M_{state},  m_{i, j} \in \{0, 1, 2, -1\},$ where $0 \leq i, j \leq 7$.

(0 means blank, and 1, 2, -1 means snake body, raspberry, and block respectively.)

(We also tried another representation that marks the snake's head as 3 in addition, which will be explained in detail at section 4.) 

### 3.2.2 Action Set
Given an action set $A$, $\forall a \in A, a \in \{0, 1, 2, 3\}.$

(0 means up, and 1, 2, -1 means left, down, and right respectively.) 

### 3.2.3 Reward Function
Given a reward function $R$ and state $s$, $R(s) \in \{ 0.1, 1, -10\}$.

The reward will be 0.1 if nothing happen this round, where if snake ate the raspberry, it will get 1 point and get -10 points if it crash the block.

## 3.3 DQN Model

### 3.3.1 Neural Work

We use a 3 convolution layers and two dense layers to fit the Q-function of DQN, where the input is the observation of each round and the output is the best action on such state considered by the neural network.


<img src="https://github.com/jacky18008/demo_pictures/blob/master/model.png?raw=true" width="100"   height="240">

### 3.3.2 Reinforcement Learning
We pick a 0.95 gamma to emphasize the immediate reward than future reward, which can also avoid the reward vanish on a longer term.

The weightsï¼ˆor the coefficients in Q-function instead) in the neural network will be updated every 10000 steps. (about 140 games)

50000 random steps will be executed before training, which will become the initial data set for learning.

We trained 200 episodes (2000000 steps) on each AI. 

# 4. Experiments


# 5. Result
 
# 6. Disscussion
 
# 7. Conclusions  